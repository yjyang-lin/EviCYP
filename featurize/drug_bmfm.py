from pathlib import Path
import sys
import os
project_root = Path(__file__).parent.parent.resolve()
sys.path.insert(0, str(project_root))

from bmfm_sm.api.smmv_api import SmallMoleculeMultiViewModel
from bmfm_sm.core.data_modules.namespace import LateFusionStrategy
from bmfm_sm.predictive.data_modules.graph_finetune_dataset import (
    Graph2dFinetuneDataPipeline,
)
from bmfm_sm.predictive.data_modules.image_finetune_dataset import (
    ImageFinetuneDataPipeline,
)
from bmfm_sm.predictive.data_modules.text_finetune_dataset import (
    TextFinetuneDataPipeline,
)
import torch
import pandas as pd
import logging
from tqdm import tqdm
import pickle
import numpy as np

# RDKit imports
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem import Descriptors
from rdkit import RDLogger
RDLogger.DisableLog('rdApp.*')


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Load model ---
model = SmallMoleculeMultiViewModel.from_pretrained(
    LateFusionStrategy.ATTENTIONAL,
    model_path='ibm-research/biomed.sm.mv-te-84m',
    huggingface=True)
model = model.to('cpu')

def get_emb(smiles):
    joint_dict = {}
    joint_dict.update(Graph2dFinetuneDataPipeline.smiles_to_graph_format(smiles))
    joint_dict.update(TextFinetuneDataPipeline.smiles_to_text_format(smiles))
    joint_dict.update(ImageFinetuneDataPipeline.smiles_to_image_format(smiles))
    joint_dict = {k: v.to('cpu') if torch.is_tensor(v) else v for k, v in joint_dict.items()}
    return model.get_embeddings(joint_dict)

# --- RDKit features: descriptors + Morgan fingerprint ---
EXCLUDED_DESCRIPTORS = {'SMR_VSA8', 'SlogP_VSA9', 'fr_isocyan', 'fr_prisulfonamd', 'Ipc'}
# Note: The feature dimension should be a multiple of 4. The descriptor 'SMR_VSA8', 'SlogP_VSA9', 'fr_isocyan', 'fr_prisulfonamd' 
# was consistently zero across all molecules. The 'Ipc' descriptor values exceeded 1e10 
# in some cases. Both descriptors have been excluded from the feature set.

# Build descriptor list from RDKit and filter exclusions
try:
    rdkit_desc_list = [(name, func) for name, func in Descriptors._descList if name not in EXCLUDED_DESCRIPTORS]
except Exception:
    rdkit_desc_list = []
    for name in dir(Descriptors):
        attr = getattr(Descriptors, name)
        if callable(attr) and not name.startswith("_"):
            rdkit_desc_list.append((name, attr))
    rdkit_desc_list = [(n, f) for n, f in rdkit_desc_list if n not in EXCLUDED_DESCRIPTORS]

descriptor_names = [name for name, _ in rdkit_desc_list]
num_descriptors = len(descriptor_names)
print(f"Number of descriptors: {num_descriptors}")
# Note: The number of molecular descriptors generated by RDKit may vary across different versions.
# This code has been developed and tested with RDKit version 2025.03.6.
# If you are using a different version, the descriptor count and order might differ,
# which could affect the feature dimensions and model performance.
# print : Number of descriptors: 212

# Load normalization parameters
def load_normalization_params(datadir):
    norm_params_path = os.path.join(datadir, 'normalization_parameters.csv')
    if not os.path.exists(norm_params_path):
        logger.warning(f"Normalization parameters file not found at {norm_params_path}")
        return {}
    
    norm_params_df = pd.read_csv(norm_params_path)
    norm_params = {}
    for _, row in norm_params_df.iterrows():
        norm_params[row['descriptor']] = (row['mean'], row['std'])
    return norm_params

def compute_rdkit_descriptors(mol, norm_params=None):
    if mol is None:
        return np.full(len(descriptor_names), np.nan, dtype=np.float32)
    
    vals = []
    for name, func in rdkit_desc_list:
        try:
            v = func(mol)
            # Apply normalization if parameters are provided and the current descriptor is in the parameter list
            if norm_params and name in norm_params:
                mean, std = norm_params[name]
                # Avoid division by zero
                if std == 0:
                    v_norm = 0.0 if v is None else float(v)
                else:
                    v_norm = (float(v) - mean) / std if v is not None else np.nan
                vals.append(v_norm)
            else:
                vals.append(float(v) if v is not None else np.nan)
        except Exception:
            vals.append(np.nan)
    return np.array(vals, dtype=np.float32)

def compute_morgan_fp(mol, radius=2, nBits=512):
    if mol is None:
        return np.zeros(nBits, dtype=np.float32)
    try:
        bv = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)
        arr = np.zeros((nBits,), dtype=np.uint8)
        AllChem.DataStructs.ConvertToNumpyArray(bv, arr)
        return arr.astype(np.float32)
    except Exception:
        return np.zeros(nBits, dtype=np.float32)

def get_rdkit_features_from_smiles(smiles, norm_params=None):
    mol = None
    try:
        mol = Chem.MolFromSmiles(smiles)
    except Exception:
        mol = None
    descriptors = compute_rdkit_descriptors(mol, norm_params)
    fingerprint = compute_morgan_fp(mol, radius=2, nBits=512)
    return descriptors, fingerprint

def read_data(datadir, sep=',', use_val_only=False):
    """
    Read data from CSV files in the specified directory.
    
    Args:
        datadir (str): Directory containing data files
        sep (str): Separator used in CSV files
        use_val_only (bool): If True, only read val.csv (for external validation sets)
    
    Returns:
        pd.Series: Unique SMILES strings
    """
    if use_val_only:
        val = pd.read_csv(f'{datadir}/val.csv', sep=sep)
        alldata = val
    else:
        train = pd.read_csv(f'{datadir}/train.csv', sep=sep)
        val = pd.read_csv(f'{datadir}/val.csv', sep=sep)
        test = pd.read_csv(f'{datadir}/test.csv', sep=sep)
        alldata = pd.concat([train, val, test])
    
    entities = alldata['SMILES'].unique()
    logger.info(f'{len(entities)} unique entities found')
    return entities

def featurize_data(entities, norm_params):
    out = {}
    nan_entities = []
    for entity in tqdm(entities):
        # Model embedding
        emb = get_emb(entity)
        emb_np = emb.cpu().detach().numpy().astype(np.float32).ravel()

        # RDKit features
        desc_vals, fp = get_rdkit_features_from_smiles(entity, norm_params)

        # Concatenate: embedding + descriptors + fingerprint
        desc_vals = desc_vals.astype(np.float32)
        fp = fp.astype(np.float32)

        concat = np.concatenate([emb_np.ravel(), desc_vals.ravel(), fp.ravel()]).astype(np.float32)

        if np.isnan(concat).any():
            nan_entities.append(entity)
            logger.warning(f"NaN detected in features for SMILES: {entity}")

        # Keep SMILES as the key, value is the concatenated vector
        out[entity] = concat
    return out, nan_entities

if __name__ == '__main__':
    feature = os.path.basename(__file__)[:-3]
    datadir = './data_random'
    logger.info(f"Reading data from {datadir}")

    # Set to True if only val.csv exists
    use_val_only = not os.path.exists(f'{datadir}/train.csv') and not os.path.exists(f'{datadir}/test.csv')

    # Load normalization parameters
    norm_params = load_normalization_params(datadir)
    logger.info(f"Loaded normalization parameters for {len(norm_params)} descriptors")
    
    entities = read_data(datadir, use_val_only=use_val_only)
    out, nan_entities = featurize_data(entities, norm_params)
 
    if nan_entities:
        logger.warning(f"Found {len(nan_entities)} entities with NaN values:")
        for i, entity in enumerate(nan_entities, 1):
            logger.warning(f"{i}. SMILES: {entity}")
    else:
        logger.info("No NaN values found in any feature vectors")
    
    save_path = f'{datadir}/{feature}.pkl'
    with open(save_path, 'wb') as f:
        pickle.dump(out, f)

    logger.info(f"Saved concatenated features for {len(out)} entities to {save_path}")